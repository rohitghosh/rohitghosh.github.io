<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Iota</title>
 <link href="http://rohitghosh.github.io//atom.xml" rel="self"/>
 <link href="http://rohitghosh.github.io//"/>
 <updated>2016-09-12T21:29:03+05:30</updated>
 <id>http://rohitghosh.github.io/</id>
 <author>
   <name>Rohit Ghosh</name>
   <email>rohitghosh177@gmail.com</email>
 </author>

 
 <entry>
   <title>AI - Beyond Buzzwords &amp; Comparisons To Human Abilities</title>
   <link href="http://rohitghosh.github.io//2016/09/05/AI-beyond-buzzwords/"/>
   <updated>2016-09-05T00:00:00+05:30</updated>
   <id>http://rohitghosh.github.io//2016/09/05/AI-beyond-buzzwords</id>
   <content type="html">&lt;p&gt;Any heated argument on AI these days soon ends up in a predictable trajectory - so DeepMind’s AlphaGo beats Go champ, Tesla to compete with Uber using AI driven cars, ethical dilemma of driver-less cars and finally - would robots overpower humankind (on sidelines of the last debate would be venue of the Matrix themed D-Day - Earth or Mars !)&lt;/p&gt;

&lt;p&gt;So if you’re writing a sci-fi book or you’re a tarot-reader then don’t bother to read further.
But, if you’re reading buzzwords everyday and wondering what’s AI, what’s the big deal about it and where we stand as of today compared to human-like capabilities of AI; may be give the post a read.&lt;/p&gt;

&lt;h3 id=&quot;what-is-ai--what-is-not-&quot;&gt;What is AI &amp;amp; what is not ?&lt;/h3&gt;

&lt;p&gt;AI, coined in 1956 at University of DartMouth by psychologist Rosenblatt,
was defined vaguely as a field aimed to make machines do the same work as humans.
In the years to follow, conventional Machine Learning (ML) algorithms became norm of the day in AI till
something strangely powerful happened in 2012. This was the ImageNet (an image recognition competition)
breakthrough by Prof. Hinton (in Google now) and his students based on Le Net, a Convolutional Net originally
designed by Le Cunn (in FB now), the-then prof at NYU.
This breakthrough was strange cause till then Convolutional Nets (CNNs) had hardly shown any usefulness
beyond hand-written character recognition (here’s a &lt;a href=&quot;https://www.youtube.com/watch?v=FwFduRA_L6Q&quot;&gt;video&lt;/a&gt; on Le Cunn training CNN
back in ‘93 at Bell Labs) and yet it was powerful  as it  entailed an explosion in AI over
the coming years in ways people in academia and industry had hardly foreseen. See the figure below.
&lt;img src=&quot;/images/newplot.png&quot; alt=&quot;NIPS &amp;amp; ICML Papers submitted&quot; title=&quot;NIPS &amp;amp; ICML Papers submitted&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To put it simply, AI agents learn from data, preferably huge amounts of data - instead of programmer himself hard-coding and modelling reactions to various use cases. The idea is&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Don’t model the World but model the Brain and only then let the modelled Brain learn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Modelled after brain, in it’s simplest avatar, a net essentially consists of a graph of nodes which processes input sequentially. By ‘processing’ I meant multiplying output from previous node by a weight matrix and applying non-linearity to the product. Learning the weight matrices is the whole game !&lt;/p&gt;

&lt;p&gt;Also, it’s not data mining where the pattern is mined from data and then requisite changes are made in the algorithm as per observed pattern. AI is more end-to-end. So when I see a lot of startups using smart but hard-coded logic based algorithms (based on data or otherwise) under alias of AI, I feel it’s gravely misleading.&lt;/p&gt;

&lt;h3 id=&quot;why-the-hype-around-ai-this-time-around-&quot;&gt;Why the hype around AI this time around ?&lt;/h3&gt;

&lt;p&gt;Interestingly, after perceptrons were introduced in 1956, it was followed by immense hype regarding it’s capabilities, sort of what we see around AI now. And sadly, there was a lot of disappointment that followed it. In fact, there were couple of AI winters in the intermediate years. (‘74-‘80 &amp;amp; ‘87-‘93)&lt;/p&gt;

&lt;p&gt;The reasons, both time around, were lack of computational power, coupled with cuts in funding because of economic crashes. Post ’93 even though it regained some lost ground following Le Cunn’s success in handwritten digit recognition, it wasn’t until 2012 that using nets became mainstream, again.&lt;/p&gt;

&lt;h4 id=&quot;so-whats-happening-right-this-time-around-&quot;&gt;So what’s happening right this time around ?&lt;/h4&gt;

&lt;p&gt;The nets that were hardly 10 layers deep pre-2012, can now reach 1000 layers easily without much trouble in training. Deeper nets kicked off the field of Deep Learning - the deeper the nets, the better the results tend to be in general. The reasons are majorly -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Higher computational powers&lt;/em&gt;: The computational gains because of advent of GPU and parallelised computation was a major force behind the boom of AI this time. In fact, parallelised computation was the recipe for Prof. Hinton’s Image Net success in 2012.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data, enormous data&lt;/em&gt;: Most importantly, the sole reason AI has seen success beyond expectations has been availability of data - something that could’ve only happened in post-internet, post-Google &amp;amp; post-FB era.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Funding&lt;/em&gt;: Apart from the above reasons, the amount of money that has been flowing intp AI in the current boom has been phenomenal- the funding for AI based startups has been pegged somewhere at $310mn for the year 2015.
&lt;img src=&quot;/images/AI_quarterly_finance_20160203.jpg&quot; alt=&quot;Funding in AI Space&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-ai-can-and-what-it-cant&quot;&gt;What AI can and what it can’t&lt;/h3&gt;

&lt;p&gt;Apparently, AI is pretty good when it comes to visual recognition tasks - take for example - a net that &lt;a href=&quot;http://cs231n.stanford.edu/reports2016/022_Report.pdf&quot;&gt;detects sentiment from facial expression&lt;/a&gt; (the GIF below, taken from YouTube video demo of HappyNet) or one that can do  stuff such as- make any &lt;a href=&quot;http://sites.skoltech.ru/compvision/projects/deepwarp/&quot;&gt;celeb gaze whichever way you like&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So when it comes to comparing with human performance in Computer Vision (object detection from images), AI has been able to recognise with 6.8 % error central objects in images compared to humans (the human being researcher Andrej Karapathy himself) with 5.1 % error. When it comes to semantically segmenting out objects from images, recent breakthroughs such as U-Nets are showing some great potential but they involve a lot of supervision costs (i.e. pixel-by-pixel labelling) except this very &lt;a href=&quot;https://arxiv.org/abs/1506.02106&quot;&gt;recent paper&lt;/a&gt; by Stanford Prof. Li Fei-Fei. Even though from advent of AI-driven cars, it could seem that AI has conquered computer vision, but it is very far from over. In fact, there are &lt;a href=&quot;http://arxiv.org/pdf/1606.03556v2.pdf&quot;&gt;papers&lt;/a&gt; which prove that machines see quite differently from human beings.&lt;/p&gt;

&lt;p&gt;In speech recognition as well, AI has been pretty good (with less than 8% error on vocabulary consisting of all alphabets and numbers). AI has been exceptional at playing strategy games such as Chess, AlphaGo. Also, at &lt;a href=&quot;http://blog.qure.ai/&quot;&gt;Qure&lt;/a&gt;, we can see AI performing better than some physicians on medical diagnosis, something also &lt;a href=&quot;http://www.wired.co.uk/article/ibm-watson-medical-doctor&quot;&gt;reported by IBM’s Watson&lt;/a&gt; as the underlying tasks involve visual recognition and segmentation tasks at their core.&lt;/p&gt;

&lt;p&gt;Even AI is able to &lt;a href=&quot;http://deepdreamgenerator.com/&quot;&gt;dream&lt;/a&gt; and generate images on its own - in a sense it can develop a sense of creativity - through a technique known as &lt;strong&gt;adversial training&lt;/strong&gt;. Though it’s not like the creativity we have - to think about objects that never existed. The kind of creativity that AI displays is on the lines of being able to predict a picture correctly when the picture’s central part is cropped, after obviously having seen similar images during training.&lt;/p&gt;

&lt;p&gt;But when it comes to basic comprehension tasks like bAbI tasks or MC Tests- answering based on comprehensions like -  “Sandra travelled to the office.Sandra went to the bathroom. Where is Sandra? “ AI still lags behind (around ~60% accuracy without supervision, that is when the AI isn’t supplied with any info on where in the comprehension to look for answer vs ~70% accuracy when it’s provided with the cues). Also, when it comes to answer questions based on picture, or  word-sense disambiguation or Named Entity Recognition ( recognising from “Jack was playing tennis yesterday” - that Jack is a person, tennis is a sport and yesterday is time) AI has not been anywhere close to human level accuracy.&lt;/p&gt;

&lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding thoughts&lt;/h2&gt;

&lt;p&gt;So that was AI-101, with a peek into world of AI and the truth behind it. So while AI can generate some music, predict the &lt;a href=&quot;http://www.mirror.co.uk/tech/meet-artificial-intelligence-knows-whos-7806263&quot;&gt;next person dying on popular TV series GoT&lt;/a&gt;, save the &lt;a href=&quot;http://www.huffingtonpost.in/entry/detecting-ignition-of-wildfires-sooner_b_8028786&quot;&gt;world from wildfire&lt;/a&gt;, &lt;a href=&quot;http://www.geekwire.com/2016/patook-llc/&quot;&gt;detect flirting from chats&lt;/a&gt; - but currently it has got some serious limitations as well. Well, Artificial Narrow Intelligence (ANI) , Artificial General Intelligence (AGI) &amp;amp; Artificial Super Intelligence (ASI) may be good for hypothetical discussions, in reality it’s probably far-fetched as of today. The boom in AI is real for sure, but the hysteria around AI not so much. And this is something important to remember because the first AI winter was a result of the immense hysteria around perceptrons followed by huge disappointment. If we want to avoid something similar this time, then all of us - researchers, practitioners, investors etc.- need to have realistic expectations in line with current state-of-art research. Obviously, AGI is not impossible in long run given the way things are changing everyday, but if you’re fearing a robot takeover soon or betting your money on AGI taking over on chat bots, it may be a good idea to just sit back and think again.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; : If you are a Deep Learning Scientist reading this, and are interested for a challenging as well as impacting role at qure.ai , write to us quickly.&lt;/p&gt;

&lt;p&gt;If you’re a startup founder reading this and need help in brainstorming the right AI path for your startup given current advances, feel free to contact me. More than happy to help !&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;P.P.S.&lt;/strong&gt; : All the opinions expressed above are my personal opinions, and don’t reflect by any means opinion of Qure or any other firm mentioned above.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Why Tinder works? Solving for X in Tinder for X (Part 2/2)</title>
   <link href="http://rohitghosh.github.io//2016/03/15/solving-for-X-in-Tinder-for-X/"/>
   <updated>2016-03-15T00:00:00+05:30</updated>
   <id>http://rohitghosh.github.io//2016/03/15/solving-for-X-in-Tinder-for-X</id>
   <content type="html">&lt;p&gt;In &lt;a href=&quot;https://www.linkedin.com/pulse/why-tinder-works-insights-how-build-addictive-products-nivedan-rathi&quot;&gt;Part 1&lt;/a&gt;, we delved into the user psychology of what makes Tinder such an addictive product and what are its takeaways for your business idea in general. We’ve all heard a lot about the Uber for X model now, where X has been abused and overused a lot (2016 is an especially interesting time for these startups). Let’s talk about the other cool thing - Tinder for X. So we ventured out to find what makes or break a Tinder for X model where X could be different industries like recruitment, shopping etc.&lt;/p&gt;

&lt;p&gt;Let’s begin with all the major criteria for any business problem to be suited for Tinder for X model&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Should connect 2 communities who need each other&lt;/li&gt;
  &lt;li&gt;Availability of almost infinite options to swipe for both communities&lt;/li&gt;
  &lt;li&gt;Any wrong decisions should have little downside&lt;/li&gt;
  &lt;li&gt;Snap judgement to proceed to the next meaningful step should be possible&lt;/li&gt;
  &lt;li&gt;Very little but highly significant information must be presented&lt;/li&gt;
  &lt;li&gt;Choices should be ideally binary &amp;amp; execution virtually zero effort (Tinder-swipe)&lt;/li&gt;
  &lt;li&gt;Should offer variability in rewards&lt;/li&gt;
  &lt;li&gt;Should trigger the emotion of ego boost from a “match”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, let’s look at some of the Tinder for X model, solving for X we get&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;X = Employer / Job-seeker | Major Apps : Super(India), Jobr&lt;/dt&gt;

&lt;dd&gt;Looking for jobs had been made easier over the years but a Tinder-like model beats everyone in engagement. There are endless startup jobs &amp;amp; an equal number of young grads on the app validate the effectiveness and need of the idea. Obviously, there’s also an ‘ego boost’ for an employee, as well as for the employer to some extent, whenever there’s a ‘match’.

However, it’s not a snap judgement for neither the employer nor the employee - it definitely takes second thoughts unless of course you’re in your notice period or the appraisal you’ve been waiting desperately for 2 years just went to someone else in the organisation. Moreover, the lack of Job Description only adds to the trouble. Also, for an employee, there’s definitely a lot of financial downside to a left swipe - that could mean a family holiday at Mauritius being just (s)wiped away!&lt;/dd&gt;

&lt;dt&gt;X = Entrepreneur / Investor | Major Apps : Tendr , PIF&lt;/dt&gt;

&lt;dd&gt;For investors sifting through a heap of startups daily, these apps are definitely a need of the hour for the investor community. Also, these apps levels field for ‘techie’ founders with limited networks. Obviously, the ‘ego boost’ exists predominantly for an entrepreneur whenever he gets a ‘right’ swipe from an investor.

Again, it would never be a snap judgement. Judging business potential isn’t as cognitive as judging people for hook-up, even for a seasoned investor. Forget business potential, even a beauty pageant where the sole job was to judge beauty by gazing- they ended up asking GK questions.

Also, an endless supply of investors would be tough. However, can’t comment same about no. of entrepreneurs in India these days. It’s only time till the Vijay Mallya raises investment for bail as well. Moreover, a high probability of financial downside for missing out a potential business idea for an investor makes snap judgement even harder.&lt;/dd&gt;

&lt;dt&gt;X = Online shopper / online store | Major Apps: Mallzee. Hit or Miss&lt;/dt&gt;

&lt;dd&gt;Definitely a must-have app for an ever deal-hungry consumer - what better than looking through an endless stack and dismissing based on % of discount. Given these days of deep-discounted customer acquisition strategy, judging based on discount is more cognitive than judging strangers.

However, this idea fails miserably on the ‘ego boost’ front. A store would never swipe left a customer, so basically it’s never a ‘match’ in the truest sense. Given a customer knows that he would always be swiped right, there’s no thrill or as such ‘variable reward’ (check the Part 1 for details)&lt;/dd&gt;

&lt;dt&gt;X = News/ Home-hunt/ Food | Major Apps : InShorts, HomeSwipe, Nibbly&lt;/dt&gt;

&lt;dd&gt;Let’s be clear at the outset, an app that gives you summarised news or affordable homes or delicious food doesn’t qualify, as we are talking about Tinder for X model. Sadly only if a news or a house or a biriyani could swipe you right and give you that ego boost then we were done here.

The lack of ‘variable rewards’ &amp;amp; ‘ego boost’ makes this model not so attractive. It’s clearly a case of wrong nomenclature when someone calls something like ‘News In Shorts’ a Tinder for X model.&lt;/dd&gt;

&lt;/dl&gt;

&lt;p&gt;To conclude, all these are frankly our opinions &amp;amp; judgements, backed by data. Any reference to any startups living or shut down are completely intentional but not ill-intended at all.&lt;/p&gt;

&lt;p&gt;We feel that any product could be tweaked to be awesome, not necessarily in Tinder model.  Please feel free to contact us - me &amp;amp; my co-author Nivedan in case you’re facing any challenge with your cool product and we can help you with designing a product strategy to make it even more awesome.&lt;/p&gt;

&lt;p&gt;We would love to know more from you about what you think are the reasons for the success of Tinder and similar products. Also, any other A for B models would be a great topic to discuss.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>So, can coding be a lot easier ?</title>
   <link href="http://rohitghosh.github.io//2016/02/09/starting-to-code/"/>
   <updated>2016-02-09T00:00:00+05:30</updated>
   <id>http://rohitghosh.github.io//2016/02/09/starting-to-code</id>
   <content type="html">&lt;p&gt;So most of us have had our share of interactions with developers, in some way or the other.
Luckily enough I have now been on both sides - and frankly the experience of creating things from scratch  as a developer has been overwhelming.
But then not being a &lt;cite&gt;‘techie’&lt;/cite&gt; from Day One has left me pondering with certain things, and in this “Hello World” of LinkedIn Posts, I take up one of them.&lt;/p&gt;

&lt;p&gt;The best part of my &lt;em&gt;tech-yatra&lt;/em&gt; has been the discovery that there’s a a very strong &amp;amp; active community around  development and StackOverflow is undoubtedly the best.
The support and exhaustiveness of the community is widespread which definitely helps.
But, as a rookie developer, 4 months ago, I was really torn apart in finding out relevant questions on StackOverflow, reading the top voted answers and then reading documentations as well whenever I faced an error statement.
To add to my woes, not all solutions were customised as per my requirement !&lt;/p&gt;

&lt;p&gt;Now, obviously there’s a joy in learning non-linearly, figuring out things as we go but sometimes these ‘blocks’can be dangerous.
In fact, while at IIT, a major reason I discontinued coding was partially because of these ‘blocks’.
As Mihaly Csikszentmihalyi, the pioneer of the concept of ‘Flow’ and author of the book by same name ( a highly recommended read for all ) explains from the figure below, our challenges  need to be proportionate to our skill level for our continuous progress  - otherwise we often get stuck up in anxiety zones  with tough problems and we never progress onto the next level.
So, if the plan is to be on a steep learning curve or learn within limited time  (say, over weekends where you can’t waste time getting stuck up and lose inspiration to progress),  is there a solution to do programming an easier way ? Not so easy that it becomes boring but easy enough to be in ‘flow’.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Introduction.jpg&quot; alt=&quot;placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What if those guys who answer your questions on StackOverflow or people like them could answer your questions personally  and instantaneously ?
What if those moments of anxiety caused by getting stuck can be overcome easily with the help from experts who have relevant credible experience in that language/software/ framework  ?
Not an online tutor  (cause that pushes coding  into ‘boring’ zone ), but a chat-based assistant whom you could delegate the task of “StackOverflow”ing  things and helping you ASAP only when you’re stuck in a problem.
And by virtue of their own expertise in the language coupled with past experience of handling similar problems from other users, the help would arrive an user’s way far more earlier than their own efforts would have !&lt;/p&gt;

&lt;p&gt;I would love to know your opinion, especially more if you’re starting out as a developer and otherwise also,
as to whether you would like to use a similar help at a monetary cost of say, less than 1k/year  and how badly would you need it for whatever reasons may be -
web, app, software development or data science applications, machine learning applications etc.&lt;/p&gt;

&lt;p&gt;Would love to hear your opinion and feel free to contact me if this sounds interesting !&lt;/p&gt;

&lt;p&gt;P.S. : Just to give a context of areas where I faced challenges, in past 4 months :  I was involved  extensively into building Machine Learning algorithms in fields of NLP, predictive analytics
while building a small Android application on the side. Currently, though I have been figuring out things on applications of  NLG and block-chain technology.&lt;/p&gt;
</content>
 </entry>
 

</feed>
